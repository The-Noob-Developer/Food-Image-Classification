{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtkWO1Mgf_4w",
        "outputId": "fa456b10-e728-4adc-9ec8-b7e4c349eab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.3)\n",
            "Requirement already satisfied: serpapi in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (0.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 serpapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K35kH6ChMPP",
        "outputId": "b276b72b-83ba-4c55-d0cf-a67891bdf05f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: google-search-results in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (2.4.2)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZBPHg3RhF4P",
        "outputId": "30ba0c8e-4540-4bff-f8c6-c598d7d84201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SerpAPI import successful!\n"
          ]
        }
      ],
      "source": [
        "from serpapi import GoogleSearch\n",
        "print(\"SerpAPI import successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLwS28ntgGs_",
        "outputId": "d998b702-3cf3-4196-8482-b463bc6a1f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching images for Pani Puri...\n",
            "Downloaded: food_images\\Pani_Puri\\Pani_Puri_1.jpg\n",
            "Downloaded: food_images\\Pani_Puri\\Pani_Puri_2.jpg\n",
            "Downloaded: food_images\\Pani_Puri\\Pani_Puri_3.jpg\n",
            "Downloaded: food_images\\Pani_Puri\\Pani_Puri_4.jpg\n",
            "Downloaded: food_images\\Pani_Puri\\Pani_Puri_5.jpg\n",
            "Downloaded: food_images\\Pani_Puri\\Pani_Puri_8.jpg\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching images for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfood\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m image_urls \u001b[38;5;241m=\u001b[39m fetch_image_urls(food, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)  \u001b[38;5;66;03m# Adjust number as needed\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m download_images(image_urls, SAVE_DIR, food)\n",
            "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mdownload_images\u001b[1;34m(image_urls, parent_folder, query)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_urls):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m         response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     31\u001b[0m             file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(item_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "# Set up your SerpAPI key\n",
        "SERPAPI_KEY = \"ef4c5669d45f215435581977f32cd2a178819c5fb9f601784e3d564c939ee94a\"  # Replace with your SerpAPI key\n",
        "SAVE_DIR = \"food_images\"\n",
        "\n",
        "def fetch_image_urls(query, num_images=100):\n",
        "    params = {\n",
        "        \"engine\": \"google_images\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERPAPI_KEY,\n",
        "        \"num\": num_images\n",
        "    }\n",
        "\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "    images = results.get(\"images_results\", [])\n",
        "\n",
        "    return [img[\"original\"] for img in images[:num_images]]\n",
        "\n",
        "def download_images(image_urls, parent_folder, query):\n",
        "    item_folder = os.path.join(parent_folder, query.replace(\" \", \"_\"))  # Create a subfolder for each food item\n",
        "    os.makedirs(item_folder, exist_ok=True)\n",
        "\n",
        "    for i, url in enumerate(image_urls):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                file_path = os.path.join(item_folder, f\"{query.replace(' ', '_')}_{i+1}.jpg\")\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"Downloaded: {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    food_items = [\n",
        "        \"Pani Puri\", \"Samosa\", \"Vada Pav\", \"Pav Bhaji\", \"Dabeli\",\n",
        "        \"Misal Pav\", \"Aloo Tikki\", \"Dahi Puri\", \"Sev Puri\", \"Bhel Puri\",\n",
        "        \"Ragda Pattice\", \"Kachori\", \"Kathi Roll\", \"Frankie Roll\", \"Momos\",\n",
        "        \"Bread Pakora\", \"Dhokla\", \"Pakoras\", \"Chana Chaat\", \"Masala Papad\",\n",
        "        \"Dosa\", \"Idli\", \"Medu Vada\", \"Uttapam\", \"Pesarattu\",\n",
        "        \"Chole Bhature\", \"Rajma Chawal\", \"Aloo Paratha\", \"Chole Kulche\", \"Tandoori Momos\",\n",
        "        \"Hakka Noodles\", \"Chilli Paneer\", \"Schezwan Fried Rice\", \"Manchurian\", \"Spring Rolls\",\n",
        "        \"Paneer Pizza\", \"Tandoori Pizza\", \"Cheese Burst Sandwich\", \"Burger\", \"French Fries\",\n",
        "        \"Maggi\", \"Grilled Sandwich\", \"Cheese Toast\", \"Veg Puff\", \"Chicken Shawarma\",\n",
        "        \"Egg Roll\", \"Fish Fry\", \"Kebab\", \"Tandoori Chaap\", \"Shawarma Wrap\"\n",
        "    ]\n",
        "\n",
        "    for food in food_items:\n",
        "        print(f\"Fetching images for {food}...\")\n",
        "        image_urls = fetch_image_urls(food, num_images=200)  # Adjust number as needed\n",
        "        download_images(image_urls, SAVE_DIR, food)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten , BatchNormalization , Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "image = cv2.imread('food_images/Aloo_Paratha/Aloo_Paratha_9.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image\u001b[38;5;241m.\u001b[39mshape()\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting duckduckgo_search\n",
            "  Downloading duckduckgo_search-7.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting click>=8.1.8 (from duckduckgo_search)\n",
            "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting primp>=0.11.0 (from duckduckgo_search)\n",
            "  Downloading primp-0.12.1-cp38-abi3-win_amd64.whl.metadata (13 kB)\n",
            "Collecting lxml>=5.3.0 (from duckduckgo_search)\n",
            "  Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click>=8.1.8->duckduckgo_search) (0.4.6)\n",
            "Downloading duckduckgo_search-7.3.2-py3-none-any.whl (19 kB)\n",
            "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl (3.8 MB)\n",
            "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.5/3.8 MB 2.8 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 1.0/3.8 MB 3.1 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 1.8/3.8 MB 3.2 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 2.4/3.8 MB 3.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 3.4/3.8 MB 3.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.8/3.8 MB 3.3 MB/s eta 0:00:00\n",
            "Downloading primp-0.12.1-cp38-abi3-win_amd64.whl (3.1 MB)\n",
            "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
            "   ---------- ----------------------------- 0.8/3.1 MB 4.8 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 1.6/3.1 MB 4.2 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 2.4/3.1 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.1/3.1 MB 3.7 MB/s eta 0:00:00\n",
            "Installing collected packages: primp, lxml, click, duckduckgo_search\n",
            "Successfully installed click-8.1.8 duckduckgo_search-7.3.2 lxml-5.3.1 primp-0.12.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script ddgs.exe is installed in 'C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "!pip install duckduckgo_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded 1/500 images\n",
            "Downloaded 2/500 images\n",
            "Downloaded 3/500 images\n",
            "Downloaded 4/500 images\n",
            "Downloaded 5/500 images\n",
            "Downloaded 6/500 images\n",
            "Downloaded 7/500 images\n",
            "Downloaded 8/500 images\n",
            "Downloaded 9/500 images\n",
            "Downloaded 10/500 images\n",
            "Downloaded 11/500 images\n",
            "Downloaded 12/500 images\n",
            "Downloaded 13/500 images\n",
            "Downloaded 14/500 images\n",
            "Downloaded 15/500 images\n",
            "Downloaded 16/500 images\n",
            "Downloaded 17/500 images\n",
            "Downloaded 18/500 images\n",
            "Downloaded 19/500 images\n",
            "Downloaded 20/500 images\n",
            "Downloaded 21/500 images\n",
            "Downloaded 22/500 images\n",
            "Downloaded 23/500 images\n",
            "Downloaded 24/500 images\n",
            "Downloaded 25/500 images\n",
            "Downloaded 26/500 images\n",
            "Downloaded 27/500 images\n",
            "Downloaded 28/500 images\n",
            "Downloaded 29/500 images\n",
            "Downloaded 30/500 images\n",
            "Downloaded 31/500 images\n",
            "Downloaded 32/500 images\n",
            "Downloaded 33/500 images\n",
            "Downloaded 34/500 images\n",
            "Downloaded 35/500 images\n",
            "Downloaded 36/500 images\n",
            "Downloaded 37/500 images\n",
            "Downloaded 38/500 images\n",
            "Downloaded 39/500 images\n",
            "Downloaded 40/500 images\n",
            "Downloaded 41/500 images\n",
            "Downloaded 42/500 images\n",
            "Downloaded 43/500 images\n",
            "Downloaded 44/500 images\n",
            "Downloaded 45/500 images\n",
            "Downloaded 46/500 images\n",
            "Downloaded 47/500 images\n",
            "Downloaded 48/500 images\n",
            "Downloaded 49/500 images\n",
            "Downloaded 50/500 images\n",
            "Downloaded 51/500 images\n",
            "Downloaded 52/500 images\n",
            "Downloaded 53/500 images\n",
            "Downloaded 54/500 images\n",
            "Downloaded 55/500 images\n",
            "Downloaded 56/500 images\n",
            "Downloaded 57/500 images\n",
            "Downloaded 58/500 images\n",
            "Downloaded 59/500 images\n",
            "Downloaded 60/500 images\n",
            "Downloaded 61/500 images\n",
            "Downloaded 62/500 images\n",
            "Downloaded 63/500 images\n",
            "Downloaded 64/500 images\n",
            "Downloaded 65/500 images\n",
            "Downloaded 66/500 images\n",
            "Downloaded 67/500 images\n",
            "Downloaded 68/500 images\n",
            "Downloaded 69/500 images\n",
            "Downloaded 70/500 images\n",
            "Downloaded 71/500 images\n",
            "Downloaded 72/500 images\n",
            "Downloaded 73/500 images\n",
            "Downloaded 74/500 images\n",
            "Downloaded 75/500 images\n",
            "Downloaded 76/500 images\n",
            "Downloaded 77/500 images\n",
            "Downloaded 78/500 images\n",
            "Downloaded 79/500 images\n",
            "Downloaded 80/500 images\n",
            "Downloaded 81/500 images\n",
            "Downloaded 82/500 images\n",
            "Downloaded 83/500 images\n",
            "Downloaded 84/500 images\n",
            "Downloaded 85/500 images\n",
            "Downloaded 86/500 images\n",
            "Downloaded 87/500 images\n",
            "Downloaded 88/500 images\n",
            "Downloaded 89/500 images\n",
            "Downloaded 90/500 images\n",
            "Downloaded 91/500 images\n",
            "Downloaded 92/500 images\n",
            "Downloaded 93/500 images\n",
            "Downloaded 94/500 images\n",
            "Downloaded 95/500 images\n",
            "Downloaded 96/500 images\n",
            "Downloaded 97/500 images\n",
            "Downloaded 98/500 images\n",
            "Downloaded 99/500 images\n",
            "Downloaded 100/500 images\n",
            "Downloaded 101/500 images\n",
            "Downloaded 102/500 images\n",
            "Downloaded 103/500 images\n",
            "Downloaded 104/500 images\n",
            "Downloaded 105/500 images\n",
            "Downloaded 106/500 images\n",
            "Downloaded 107/500 images\n",
            "Downloaded 108/500 images\n",
            "Downloaded 109/500 images\n",
            "Downloaded 110/500 images\n",
            "Downloaded 111/500 images\n",
            "Downloaded 112/500 images\n",
            "Downloaded 113/500 images\n",
            "Downloaded 114/500 images\n",
            "Downloaded 115/500 images\n",
            "Downloaded 116/500 images\n",
            "Downloaded 117/500 images\n",
            "Downloaded 118/500 images\n",
            "Downloaded 119/500 images\n",
            "Downloaded 120/500 images\n",
            "Downloaded 121/500 images\n",
            "Downloaded 122/500 images\n",
            "Downloaded 123/500 images\n",
            "Downloaded 124/500 images\n",
            "Downloaded 125/500 images\n",
            "Downloaded 126/500 images\n",
            "Downloaded 127/500 images\n",
            "Downloaded 128/500 images\n",
            "Downloaded 129/500 images\n",
            "Downloaded 130/500 images\n",
            "Downloaded 131/500 images\n",
            "Downloaded 132/500 images\n",
            "Downloaded 133/500 images\n",
            "Downloaded 134/500 images\n",
            "Downloaded 135/500 images\n",
            "Downloaded 136/500 images\n",
            "Downloaded 137/500 images\n",
            "Downloaded 138/500 images\n",
            "Downloaded 139/500 images\n",
            "Downloaded 140/500 images\n",
            "Downloaded 141/500 images\n",
            "Downloaded 142/500 images\n",
            "Downloaded 143/500 images\n",
            "Downloaded 144/500 images\n",
            "Downloaded 145/500 images\n",
            "Downloaded 146/500 images\n",
            "Downloaded 147/500 images\n",
            "Downloaded 148/500 images\n",
            "Downloaded 149/500 images\n",
            "Downloaded 150/500 images\n",
            "Downloaded 151/500 images\n",
            "Downloaded 152/500 images\n",
            "Downloaded 153/500 images\n",
            "Downloaded 154/500 images\n",
            "Downloaded 155/500 images\n",
            "Downloaded 156/500 images\n",
            "Downloaded 157/500 images\n",
            "Downloaded 158/500 images\n",
            "Downloaded 159/500 images\n",
            "Downloaded 160/500 images\n",
            "Downloaded 161/500 images\n",
            "Downloaded 162/500 images\n",
            "Downloaded 163/500 images\n",
            "Downloaded 164/500 images\n",
            "Downloaded 165/500 images\n",
            "Downloaded 166/500 images\n",
            "Downloaded 167/500 images\n",
            "Downloaded 168/500 images\n",
            "Downloaded 169/500 images\n",
            "Downloaded 170/500 images\n",
            "Downloaded 171/500 images\n",
            "Downloaded 172/500 images\n",
            "Downloaded 173/500 images\n",
            "Downloaded 174/500 images\n",
            "Downloaded 175/500 images\n",
            "Downloaded 176/500 images\n",
            "Downloaded 177/500 images\n",
            "Downloaded 178/500 images\n",
            "Downloaded 179/500 images\n",
            "Downloaded 180/500 images\n",
            "Downloaded 181/500 images\n",
            "Downloaded 182/500 images\n",
            "Downloaded 183/500 images\n",
            "Downloaded 184/500 images\n",
            "Downloaded 185/500 images\n",
            "Downloaded 186/500 images\n",
            "Downloaded 187/500 images\n",
            "Downloaded 188/500 images\n",
            "Downloaded 189/500 images\n",
            "Downloaded 190/500 images\n",
            "Downloaded 191/500 images\n",
            "Downloaded 192/500 images\n",
            "Downloaded 193/500 images\n",
            "Downloaded 194/500 images\n",
            "Downloaded 195/500 images\n",
            "Downloaded 196/500 images\n",
            "Downloaded 197/500 images\n",
            "Downloaded 198/500 images\n",
            "Downloaded 199/500 images\n",
            "Downloaded 200/500 images\n",
            "Downloaded 201/500 images\n",
            "Downloaded 202/500 images\n",
            "Downloaded 203/500 images\n",
            "Downloaded 204/500 images\n",
            "Downloaded 205/500 images\n",
            "Downloaded 206/500 images\n",
            "Downloaded 207/500 images\n",
            "Downloaded 208/500 images\n",
            "Downloaded 209/500 images\n",
            "Downloaded 210/500 images\n",
            "Downloaded 211/500 images\n",
            "Downloaded 212/500 images\n",
            "Downloaded 213/500 images\n",
            "Downloaded 214/500 images\n",
            "Downloaded 215/500 images\n",
            "Downloaded 216/500 images\n",
            "Failed to download http://www.webistinc.net/websites/e8829a46-3909-4c30-ab59-0959649bfc90/widgets-data/recipes/ef4144be-bb20-4e66-8aea-66b24a1d3786.jpg: HTTPConnectionPool(host='www.webistinc.net', port=80): Max retries exceeded with url: /websites/e8829a46-3909-4c30-ab59-0959649bfc90/widgets-data/recipes/ef4144be-bb20-4e66-8aea-66b24a1d3786.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x0000022F8D7526F0>: Failed to resolve 'www.webistinc.net' ([Errno 11001] getaddrinfo failed)\"))\n",
            "Downloaded 217/500 images\n",
            "Downloaded 218/500 images\n",
            "Downloaded 219/500 images\n",
            "Downloaded 220/500 images\n",
            "Downloaded 221/500 images\n",
            "Downloaded 222/500 images\n",
            "Downloaded 223/500 images\n",
            "Downloaded 224/500 images\n",
            "Downloaded 225/500 images\n",
            "Downloaded 226/500 images\n",
            "Downloaded 227/500 images\n",
            "Downloaded 228/500 images\n",
            "Downloaded 229/500 images\n",
            "Downloaded 230/500 images\n",
            "Downloaded 231/500 images\n",
            "Downloaded 232/500 images\n",
            "Downloaded 233/500 images\n",
            "Downloaded 234/500 images\n",
            "Downloaded 235/500 images\n",
            "Downloaded 236/500 images\n",
            "Downloaded 237/500 images\n",
            "Downloaded 238/500 images\n",
            "Downloaded 239/500 images\n",
            "Downloaded 240/500 images\n",
            "Downloaded 241/500 images\n",
            "Downloaded 242/500 images\n",
            "Downloaded 243/500 images\n",
            "Downloaded 244/500 images\n",
            "Downloaded 245/500 images\n",
            "Downloaded 246/500 images\n",
            "Downloaded 247/500 images\n",
            "Downloaded 248/500 images\n",
            "Downloaded 249/500 images\n",
            "Downloaded 250/500 images\n",
            "Downloaded 251/500 images\n",
            "Downloaded 252/500 images\n",
            "Downloaded 253/500 images\n",
            "Downloaded 254/500 images\n",
            "Downloaded 255/500 images\n",
            "Downloaded 256/500 images\n",
            "Downloaded 257/500 images\n",
            "Downloaded 258/500 images\n",
            "Downloaded 259/500 images\n",
            "Downloaded 260/500 images\n",
            "Downloaded 261/500 images\n",
            "Downloaded 262/500 images\n",
            "Downloaded 263/500 images\n",
            "Downloaded 264/500 images\n",
            "Downloaded 265/500 images\n",
            "Downloaded 266/500 images\n",
            "Downloaded 267/500 images\n",
            "Downloaded 268/500 images\n",
            "Downloaded 269/500 images\n",
            "Downloaded 270/500 images\n",
            "Downloaded 271/500 images\n",
            "Downloaded 272/500 images\n",
            "Downloaded 273/500 images\n",
            "Downloaded 274/500 images\n",
            "Downloaded 275/500 images\n",
            "Downloaded 276/500 images\n",
            "Downloaded 277/500 images\n",
            "Downloaded 278/500 images\n",
            "Downloaded 279/500 images\n",
            "Downloaded 280/500 images\n",
            "Downloaded 281/500 images\n",
            "Downloaded 282/500 images\n",
            "Downloaded 283/500 images\n",
            "Downloaded 284/500 images\n",
            "Downloaded 285/500 images\n",
            "Downloaded 286/500 images\n",
            "Downloaded 287/500 images\n",
            "Downloaded 288/500 images\n",
            "Downloaded 289/500 images\n",
            "Downloaded 290/500 images\n",
            "Downloaded 291/500 images\n",
            "Downloaded 292/500 images\n",
            "Downloaded 293/500 images\n",
            "Downloaded 294/500 images\n",
            "Downloaded 295/500 images\n",
            "Downloaded 296/500 images\n",
            "Downloaded 297/500 images\n",
            "Downloaded 298/500 images\n",
            "Downloaded 299/500 images\n",
            "Downloaded 300/500 images\n",
            "Downloaded 301/500 images\n",
            "Downloaded 302/500 images\n",
            "Downloaded 303/500 images\n",
            "Downloaded 304/500 images\n",
            "Downloaded 305/500 images\n",
            "Downloaded 306/500 images\n",
            "Downloaded 307/500 images\n",
            "Downloaded 308/500 images\n",
            "Downloaded 309/500 images\n",
            "Downloaded 310/500 images\n",
            "Downloaded 311/500 images\n",
            "Downloaded 312/500 images\n",
            "Downloaded 313/500 images\n",
            "Downloaded 314/500 images\n",
            "Downloaded 315/500 images\n",
            "Downloaded 316/500 images\n",
            "Downloaded 317/500 images\n",
            "Downloaded 318/500 images\n",
            "Downloaded 319/500 images\n",
            "Downloaded 320/500 images\n",
            "Downloaded 321/500 images\n",
            "Downloaded 322/500 images\n",
            "Downloaded 323/500 images\n",
            "Downloaded 324/500 images\n",
            "Downloaded 325/500 images\n",
            "Downloaded 326/500 images\n",
            "Downloaded 327/500 images\n",
            "Downloaded 328/500 images\n",
            "Downloaded 329/500 images\n",
            "Downloaded 330/500 images\n",
            "Downloaded 331/500 images\n",
            "Downloaded 332/500 images\n",
            "Downloaded 333/500 images\n",
            "Downloaded 334/500 images\n",
            "Downloaded 335/500 images\n",
            "Downloaded 336/500 images\n",
            "Downloaded 337/500 images\n",
            "Downloaded 338/500 images\n",
            "Downloaded 339/500 images\n",
            "Downloaded 340/500 images\n",
            "Downloaded 341/500 images\n",
            "Downloaded 342/500 images\n",
            "Downloaded 343/500 images\n",
            "Downloaded 344/500 images\n",
            "Downloaded 345/500 images\n",
            "Downloaded 346/500 images\n",
            "Downloaded 347/500 images\n",
            "Downloaded 348/500 images\n",
            "Downloaded 349/500 images\n",
            "Downloaded 350/500 images\n",
            "Downloaded 351/500 images\n",
            "Downloaded 352/500 images\n",
            "Downloaded 353/500 images\n",
            "Downloaded 354/500 images\n",
            "Downloaded 355/500 images\n",
            "Downloaded 356/500 images\n",
            "Downloaded 357/500 images\n",
            "Downloaded 358/500 images\n",
            "Downloaded 359/500 images\n",
            "Downloaded 360/500 images\n",
            "Downloaded 361/500 images\n",
            "Downloaded 362/500 images\n",
            "Downloaded 363/500 images\n",
            "Downloaded 364/500 images\n",
            "Downloaded 365/500 images\n",
            "Downloaded 366/500 images\n",
            "Downloaded 367/500 images\n",
            "Downloaded 368/500 images\n",
            "Downloaded 369/500 images\n",
            "Downloaded 370/500 images\n",
            "Downloaded 371/500 images\n",
            "Downloaded 372/500 images\n",
            "Downloaded 373/500 images\n",
            "Downloaded 374/500 images\n",
            "Downloaded 375/500 images\n",
            "Downloaded 376/500 images\n",
            "Downloaded 377/500 images\n",
            "Downloaded 378/500 images\n",
            "Downloaded 379/500 images\n",
            "Downloaded 380/500 images\n",
            "Downloaded 381/500 images\n",
            "Downloaded 382/500 images\n",
            "Downloaded 383/500 images\n",
            "Downloaded 384/500 images\n",
            "Downloaded 385/500 images\n",
            "Downloaded 386/500 images\n",
            "Downloaded 387/500 images\n",
            "Downloaded 388/500 images\n",
            "Downloaded 389/500 images\n",
            "Downloaded 390/500 images\n",
            "Downloaded 391/500 images\n",
            "Downloaded 392/500 images\n",
            "Downloaded 393/500 images\n",
            "Downloaded 394/500 images\n",
            "Downloaded 395/500 images\n",
            "Downloaded 396/500 images\n",
            "Downloaded 397/500 images\n",
            "Failed to download https://pandareviewz.com/wp-content/uploads/2018/08/Mangesh-Vada-Pav.jpg: HTTPSConnectionPool(host='pandareviewz.com', port=443): Read timed out. (read timeout=5)\n",
            "Downloaded 398/500 images\n",
            "Downloaded 399/500 images\n",
            "Downloaded 400/500 images\n",
            "Downloaded 401/500 images\n",
            "Downloaded 402/500 images\n",
            "Downloaded 403/500 images\n",
            "Downloaded 404/500 images\n",
            "Downloaded 405/500 images\n",
            "Downloaded 406/500 images\n",
            "Downloaded 407/500 images\n",
            "Downloaded 408/500 images\n",
            "Downloaded 409/500 images\n",
            "Downloaded 410/500 images\n",
            "Downloaded 411/500 images\n",
            "Downloaded 412/500 images\n",
            "Downloaded 413/500 images\n",
            "Downloaded 414/500 images\n",
            "Downloaded 415/500 images\n",
            "Downloaded 416/500 images\n",
            "Downloaded 417/500 images\n",
            "Downloaded 418/500 images\n",
            "Downloaded 419/500 images\n",
            "Downloaded 420/500 images\n",
            "Downloaded 421/500 images\n",
            "Downloaded 422/500 images\n",
            "Downloaded 423/500 images\n",
            "Downloaded 424/500 images\n",
            "Downloaded 425/500 images\n",
            "Downloaded 426/500 images\n",
            "Downloaded 427/500 images\n",
            "Downloaded 428/500 images\n",
            "Downloaded 429/500 images\n",
            "Downloaded 430/500 images\n",
            "Downloaded 431/500 images\n",
            "Downloaded 432/500 images\n",
            "Downloaded 433/500 images\n",
            "Downloaded 434/500 images\n",
            "Downloaded 435/500 images\n",
            "Downloaded 436/500 images\n",
            "Downloaded 437/500 images\n",
            "Downloaded 438/500 images\n",
            "Downloaded 439/500 images\n",
            "Downloaded 440/500 images\n",
            "Downloaded 441/500 images\n",
            "Downloaded 442/500 images\n",
            "Downloaded 443/500 images\n",
            "Downloaded 444/500 images\n",
            "Downloaded 445/500 images\n",
            "Downloaded 446/500 images\n",
            "Downloaded 447/500 images\n",
            "Downloaded 448/500 images\n",
            "Downloaded 449/500 images\n",
            "Downloaded 450/500 images\n",
            "Downloaded 451/500 images\n",
            "Downloaded 452/500 images\n",
            "Downloaded 453/500 images\n",
            "Downloaded 454/500 images\n",
            "Downloaded 455/500 images\n",
            "Downloaded 456/500 images\n",
            "Downloaded 457/500 images\n",
            "Downloaded 458/500 images\n",
            "Downloaded 459/500 images\n",
            "Downloaded 460/500 images\n",
            "Downloaded 461/500 images\n",
            "Downloaded 462/500 images\n",
            "Downloaded 463/500 images\n",
            "Downloaded 464/500 images\n",
            "Downloaded 465/500 images\n",
            "Downloaded 466/500 images\n",
            "Downloaded 467/500 images\n",
            "Downloaded 468/500 images\n",
            "Downloaded 469/500 images\n",
            "Downloaded 470/500 images\n",
            "Downloaded 471/500 images\n",
            "Downloaded 472/500 images\n",
            "Downloaded 473/500 images\n",
            "Downloaded 474/500 images\n",
            "Downloaded 475/500 images\n",
            "Downloaded 476/500 images\n",
            "Downloaded 477/500 images\n",
            "Downloaded 478/500 images\n",
            "Downloaded 479/500 images\n",
            "Downloaded 480/500 images\n",
            "Downloaded 481/500 images\n",
            "Downloaded 482/500 images\n",
            "Downloaded 483/500 images\n",
            "Downloaded 484/500 images\n",
            "Downloaded 485/500 images\n",
            "Downloaded 486/500 images\n",
            "Downloaded 487/500 images\n",
            "Downloaded 488/500 images\n",
            "Downloaded 489/500 images\n",
            "Downloaded 490/500 images\n",
            "Downloaded 491/500 images\n",
            "Downloaded 492/500 images\n",
            "Downloaded 493/500 images\n",
            "Downloaded 494/500 images\n",
            "Downloaded 495/500 images\n",
            "Downloaded 496/500 images\n",
            "Downloaded 497/500 images\n",
            "Downloaded 498/500 images\n",
            "Downloaded 499/500 images\n",
            "Downloaded 500/500 images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import requests\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def download_image(url, folder, count):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            with open(os.path.join(folder, f\"image_{count}.jpg\"), \"wb\") as file:\n",
        "                file.write(response.content)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "def fetch_and_download_images(categories, images_per_category=20, total_images=500):\n",
        "    os.makedirs(\"test/food_images\", exist_ok=True)\n",
        "    count = 0\n",
        "    \n",
        "    while count < total_images:\n",
        "        category = random.choice(categories)\n",
        "        category_folder = os.path.join(\"test/food_images\", category.replace(\" \", \"_\"))\n",
        "        os.makedirs(category_folder, exist_ok=True)\n",
        "        \n",
        "        with DDGS() as ddgs:\n",
        "            results = list(ddgs.images(category, max_results=images_per_category))\n",
        "            random.shuffle(results)\n",
        "            \n",
        "            for result in results:\n",
        "                if count >= total_images:\n",
        "                    break\n",
        "                download_image(result[\"image\"], category_folder, count)\n",
        "                count += 1\n",
        "                print(f\"Downloaded {count}/{total_images} images\")\n",
        "\n",
        "categories = [\n",
        "    \"Pani Puri\", \"Samosa\", \"Vada Pav\", \"Pav Bhaji\", \"Dabeli\",\n",
        "    \"Misal Pav\", \"Aloo Tikki\", \"Dahi Puri\", \"Sev Puri\", \"Bhel Puri\",\n",
        "    \"Ragda Pattice\", \"Kachori\", \"Kathi Roll\", \"Frankie Roll\", \"Momos\",\n",
        "    \"Bread Pakora\", \"Dhokla\", \"Pakoras\", \"Chana Chaat\", \"Masala Papad\",\n",
        "    \"Dosa\", \"Idli\", \"Medu Vada\", \"Uttapam\", \"Pesarattu\",\n",
        "    \"Chole Bhature\", \"Rajma Chawal\", \"Aloo Paratha\", \"Chole Kulche\", \"Tandoori Momos\",\n",
        "    \"Hakka Noodles\", \"Chilli Paneer\", \"Schezwan Fried Rice\", \"Manchurian\", \"Spring Rolls\",\n",
        "    \"Paneer Pizza\", \"Tandoori Pizza\", \"Cheese Burst Sandwich\", \"Burger\", \"French Fries\",\n",
        "    \"Maggi\", \"Grilled Sandwich\", \"Cheese Toast\", \"Veg Puff\", \"Chicken Shawarma\",\n",
        "    \"Egg Roll\", \"Fish Fry\", \"Kebab\", \"Tandoori Chaap\", \"Shawarma Wrap\"\n",
        "]\n",
        "\n",
        "fetch_and_download_images(categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All images have been moved to: test/all_food_images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_folder = \"test/food_images\"\n",
        "destination_folder = \"test/all_food_images\"\n",
        "\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "for category in os.listdir(source_folder):\n",
        "    category_path = os.path.join(source_folder, category)\n",
        "    if os.path.isdir(category_path):\n",
        "        for image in os.listdir(category_path):\n",
        "            source_path = os.path.join(category_path, image)\n",
        "            dest_path = os.path.join(destination_folder, image)\n",
        "            shutil.move(source_path, dest_path)\n",
        "\n",
        "print(\"All images have been moved to:\", destination_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Cleaning complete! All invalid images are removed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "def delete_invalid_images(directory):\n",
        "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
        "    \n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            img_path = os.path.join(root, file)\n",
        "\n",
        "            # Skip valid images\n",
        "            if file.lower().endswith(valid_extensions):\n",
        "                try:\n",
        "                    # OpenCV check\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is None:\n",
        "                        print(f\"❌ Deleting corrupt image (OpenCV failed): {img_path}\")\n",
        "                        os.remove(img_path)\n",
        "                        continue\n",
        "                    \n",
        "                    # PIL check\n",
        "                    with Image.open(img_path) as img:\n",
        "                        img.verify()  # Verifies if it's a readable image\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Deleting corrupt image (PIL failed): {img_path} - {e}\")\n",
        "                    os.remove(img_path)\n",
        "                continue\n",
        "            \n",
        "            # Delete unsupported or non-image files\n",
        "            print(f\"❌ Deleting unsupported file: {img_path}\")\n",
        "            os.remove(img_path)\n",
        "\n",
        "# Clean both train and test datasets\n",
        "delete_invalid_images('train')\n",
        "delete_invalid_images('test')\n",
        "\n",
        "print(\"✅ Cleaning complete! All invalid images are removed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Deleting corrupt image (OpenCV check failed): train\\Aloo_Paratha\\Aloo_Paratha_50.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Aloo_Paratha\\Aloo_Paratha_96.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Bhel_Puri\\Bhel_Puri_50.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Bhel_Puri\\Bhel_Puri_68.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Bhel_Puri\\Bhel_Puri_85.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Bread_Pakora\\Bread_Pakora_83.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Bread_Pakora\\Bread_Pakora_91.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Chana_Chaat\\Chana_Chaat_26.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_100.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_12.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_15.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_33.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_45.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_51.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_56.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_82.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_9.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Burst_Sandwich\\Cheese_Burst_Sandwich_93.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Cheese_Toast\\Cheese_Toast_70.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Chicken_Shawarma\\Chicken_Shawarma_72.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Chole_Bhature\\Chole_Bhature_54.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Chole_Kulche\\Chole_Kulche_20.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Chole_Kulche\\Chole_Kulche_32.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Chole_Kulche\\Chole_Kulche_46.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Chole_Kulche\\Chole_Kulche_64.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Chole_Kulche\\Chole_Kulche_75.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Chole_Kulche\\Chole_Kulche_9.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Dabeli\\Dabeli_33.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Dabeli\\Dabeli_53.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Dahi_Puri\\Dahi_Puri_43.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Frankie_Roll\\Frankie_Roll_31.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Frankie_Roll\\Frankie_Roll_38.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Frankie_Roll\\Frankie_Roll_53.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Frankie_Roll\\Frankie_Roll_71.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\French_Fries\\French_Fries_36.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\French_Fries\\French_Fries_59.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Grilled_Sandwich\\Grilled_Sandwich_75.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Grilled_Sandwich\\Grilled_Sandwich_94.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Kachori\\Kachori_52.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Masala_Papad\\Masala_Papad_32.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Medu_Vada\\Medu_Vada_71.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Misal_Pav\\Misal_Pav_84.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Momos\\Momos_79.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Paneer_Pizza\\Paneer_Pizza_55.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Paneer_Pizza\\Paneer_Pizza_64.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Paneer_Pizza\\Paneer_Pizza_85.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Pesarattu\\Pesarattu_94.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Pesarattu\\Pesarattu_98.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Rajma_Chawal\\Rajma_Chawal_21.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Rajma_Chawal\\Rajma_Chawal_53.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Sev_Puri\\Sev_Puri_52.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Sev_Puri\\Sev_Puri_87.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Chaap\\Tandoori_Chaap_21.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Chaap\\Tandoori_Chaap_29.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Chaap\\Tandoori_Chaap_43.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Chaap\\Tandoori_Chaap_48.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Chaap\\Tandoori_Chaap_86.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Chaap\\Tandoori_Chaap_9.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Chaap\\Tandoori_Chaap_92.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Momos\\Tandoori_Momos_18.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Momos\\Tandoori_Momos_28.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Momos\\Tandoori_Momos_30.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Momos\\Tandoori_Momos_38.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Momos\\Tandoori_Momos_56.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Momos\\Tandoori_Momos_61.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Momos\\Tandoori_Momos_90.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Pizza\\Tandoori_Pizza_47.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Pizza\\Tandoori_Pizza_49.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Pizza\\Tandoori_Pizza_59.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Pizza\\Tandoori_Pizza_63.jpg\n",
            "❌ Deleting corrupt image (OpenCV check failed): train\\Tandoori_Pizza\\Tandoori_Pizza_82.jpg\n",
            "✅ Cleaning complete! All invalid images are removed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "def delete_corrupt_images(directory):\n",
        "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
        "    \n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            img_path = os.path.join(root, file)\n",
        "            \n",
        "            # Check if file has a valid image extension\n",
        "            if not file.lower().endswith(valid_extensions):\n",
        "                print(f\"❌ Deleting non-image file: {img_path}\")\n",
        "                os.remove(img_path)\n",
        "                continue\n",
        "            \n",
        "            # Check using OpenCV\n",
        "            try:\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is None:\n",
        "                    print(f\"❌ Deleting corrupt image (OpenCV check failed): {img_path}\")\n",
        "                    os.remove(img_path)\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error reading {img_path} with OpenCV: {e}\")\n",
        "                os.remove(img_path)\n",
        "                continue\n",
        "\n",
        "            # Check using PIL\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    img.verify()  # Verifies if it's a readable image\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Deleting corrupt image (PIL check failed): {img_path} - {e}\")\n",
        "                os.remove(img_path)\n",
        "\n",
        "# Run the function on both datasets\n",
        "delete_corrupt_images('train')\n",
        "delete_corrupt_images('test')\n",
        "\n",
        "print(\"✅ Cleaning complete! All invalid images are removed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ No invalid files found. Your dataset is clean!\n",
            "✅ No invalid files found. Your dataset is clean!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "def check_images(directory):\n",
        "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
        "    invalid_files = []\n",
        "    \n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            img_path = os.path.join(root, file)\n",
        "            \n",
        "            # Check for unsupported file types\n",
        "            if not file.lower().endswith(valid_extensions):\n",
        "                print(f\"❌ Unsupported file: {img_path}\")\n",
        "                invalid_files.append(img_path)\n",
        "                continue\n",
        "\n",
        "            # Check for corrupt images using OpenCV\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"❌ Corrupt image (OpenCV failed): {img_path}\")\n",
        "                invalid_files.append(img_path)\n",
        "                continue\n",
        "\n",
        "            # Check for corrupt images using PIL\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    img.verify()\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Corrupt image (PIL failed): {img_path} - {e}\")\n",
        "                invalid_files.append(img_path)\n",
        "\n",
        "    if invalid_files:\n",
        "        print(\"\\n🛑 Found invalid files. Deleting now...\\n\")\n",
        "        for file in invalid_files:\n",
        "            os.remove(file)\n",
        "        print(\"✅ Invalid files removed. Try reloading your dataset.\")\n",
        "    else:\n",
        "        print(\"✅ No invalid files found. Your dataset is clean!\")\n",
        "\n",
        "# Run on train and test directories\n",
        "check_images('train')\n",
        "check_images('test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3864 files belonging to 50 classes.\n",
            "Found 396 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='train',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',  # Change from 'int' to 'categorical'\n",
        "    batch_size=128,\n",
        "    image_size=(128, 128)\n",
        ")\n",
        "\n",
        "test_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='test',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',  # Change from 'int' to 'categorical'\n",
        "    batch_size=128,\n",
        "    image_size=(128, 128)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'_PrefetchDataset' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_ds\u001b[38;5;241m.\u001b[39mshape()\n",
            "\u001b[1;31mAttributeError\u001b[0m: '_PrefetchDataset' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "train_ds.shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process(image, label):\n",
        "    image = tf.cast(image / 255.0, tf.float32)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(process)\n",
        "test_ds = test_ds.map(process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3,3), padding='valid', activation='relu', input_shape=(128, 128, 3))) # Layer 1\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3,3), padding='valid', activation='relu')) # Layer 2\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3,3), padding='valid', activation='relu')) # Layer 3\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(50, activation='softmax'))  # Output layer for 50 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,250</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m3,250\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,317,042</span> (12.65 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,317,042\u001b[0m (12.65 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,316,594</span> (12.65 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,316,594\u001b[0m (12.65 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nUnknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_3962]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_ds , epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m , validation_data \u001b[38;5;241m=\u001b[39m test_ds)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nUnknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_3962]"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_ds , epochs = 10 , validation_data = test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NEW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3111 images belonging to 50 classes.\n",
            "Found 753 images belonging to 50 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 2s/step - accuracy: 0.0245 - loss: 3.9382 - val_accuracy: 0.0372 - val_loss: 3.8924\n",
            "Epoch 2/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.0350 - loss: 3.8900 - val_accuracy: 0.0345 - val_loss: 3.8329\n",
            "Epoch 3/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 2s/step - accuracy: 0.0395 - loss: 3.8429 - val_accuracy: 0.0385 - val_loss: 3.7984\n",
            "Epoch 4/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2s/step - accuracy: 0.0456 - loss: 3.8232 - val_accuracy: 0.0544 - val_loss: 3.7348\n",
            "Epoch 5/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2s/step - accuracy: 0.0381 - loss: 3.7963 - val_accuracy: 0.0571 - val_loss: 3.7356\n",
            "Epoch 6/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 2s/step - accuracy: 0.0509 - loss: 3.7502 - val_accuracy: 0.0730 - val_loss: 3.6748\n",
            "Epoch 7/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 2s/step - accuracy: 0.0640 - loss: 3.7318 - val_accuracy: 0.0624 - val_loss: 3.6700\n",
            "Epoch 8/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2s/step - accuracy: 0.0551 - loss: 3.6974 - val_accuracy: 0.0558 - val_loss: 3.7284\n",
            "Epoch 9/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 2s/step - accuracy: 0.0622 - loss: 3.7089 - val_accuracy: 0.0797 - val_loss: 3.6131\n",
            "Epoch 10/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 2s/step - accuracy: 0.0683 - loss: 3.6375 - val_accuracy: 0.0797 - val_loss: 3.5820\n",
            "Epoch 11/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 2s/step - accuracy: 0.0856 - loss: 3.6194 - val_accuracy: 0.0717 - val_loss: 3.6503\n",
            "Epoch 12/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2s/step - accuracy: 0.0876 - loss: 3.5582 - val_accuracy: 0.1182 - val_loss: 3.4409\n",
            "Epoch 13/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 2s/step - accuracy: 0.0935 - loss: 3.5182 - val_accuracy: 0.1235 - val_loss: 3.4212\n",
            "Epoch 14/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 2s/step - accuracy: 0.1062 - loss: 3.5058 - val_accuracy: 0.1049 - val_loss: 3.3823\n",
            "Epoch 15/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 2s/step - accuracy: 0.1133 - loss: 3.4369 - val_accuracy: 0.1049 - val_loss: 3.3965\n",
            "Epoch 16/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 2s/step - accuracy: 0.1059 - loss: 3.4171 - val_accuracy: 0.1195 - val_loss: 3.3263\n",
            "Epoch 17/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 2s/step - accuracy: 0.1062 - loss: 3.3955 - val_accuracy: 0.1315 - val_loss: 3.2668\n",
            "Epoch 18/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2s/step - accuracy: 0.1247 - loss: 3.3768 - val_accuracy: 0.1262 - val_loss: 3.3222\n",
            "Epoch 19/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 2s/step - accuracy: 0.1169 - loss: 3.3292 - val_accuracy: 0.1620 - val_loss: 3.2092\n",
            "Epoch 20/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 2s/step - accuracy: 0.1343 - loss: 3.3049 - val_accuracy: 0.1434 - val_loss: 3.2148\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model training completed and saved as image_classifier_model.h5\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "data_dir = \"train\"  # Change this to your dataset location\n",
        "\n",
        "# Image parameters\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data Augmentation and Preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255, \n",
        "    rotation_range=20, \n",
        "    width_shift_range=0.2, \n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2, \n",
        "    zoom_range=0.2, \n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # 20% validation split\n",
        ")\n",
        "\n",
        "# Load datasets\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',  # Multi-class classification\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = len(train_generator.class_indices)\n",
        "\n",
        "# Define CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save(\"image_classifier_model.h5\")\n",
        "\n",
        "print(\"Model training completed and saved as image_classifier_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "Predicted Class: Fish Fry\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model(\"image_classifier_model.h5\")\n",
        "\n",
        "# Define class labels\n",
        "class_labels = {\n",
        "    0: \"Pani Puri\", 1: \"Samosa\", 2: \"Vada Pav\", 3: \"Pav Bhaji\", 4: \"Dabeli\",\n",
        "    5: \"Misal Pav\", 6: \"Aloo Tikki\", 7: \"Dahi Puri\", 8: \"Sev Puri\", 9: \"Bhel Puri\",\n",
        "    10: \"Ragda Pattice\", 11: \"Kachori\", 12: \"Kathi Roll\", 13: \"Frankie Roll\", 14: \"Momos\",\n",
        "    15: \"Bread Pakora\", 16: \"Dhokla\", 17: \"Pakoras\", 18: \"Chana Chaat\", 19: \"Masala Papad\",\n",
        "    20: \"Dosa\", 21: \"Idli\", 22: \"Medu Vada\", 23: \"Uttapam\", 24: \"Pesarattu\",\n",
        "    25: \"Chole Bhature\", 26: \"Rajma Chawal\", 27: \"Aloo Paratha\", 28: \"Chole Kulche\", 29: \"Tandoori Momos\",\n",
        "    30: \"Hakka Noodles\", 31: \"Chilli Paneer\", 32: \"Schezwan Fried Rice\", 33: \"Manchurian\", 34: \"Spring Rolls\",\n",
        "    35: \"Paneer Pizza\", 36: \"Tandoori Pizza\", 37: \"Cheese Burst Sandwich\", 38: \"Burger\", 39: \"French Fries\",\n",
        "    40: \"Maggi\", 41: \"Grilled Sandwich\", 42: \"Cheese Toast\", 43: \"Veg Puff\", 44: \"Chicken Shawarma\",\n",
        "    45: \"Egg Roll\", 46: \"Fish Fry\", 47: \"Kebab\", 48: \"Tandoori Chaap\", 49: \"Shawarma Wrap\"\n",
        "}\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(img_path):\n",
        "    IMG_SIZE = (128, 128)  # Must match training image size\n",
        "    \n",
        "    # Load the image\n",
        "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
        "    \n",
        "    # Convert to numpy array and normalize\n",
        "    img_array = image.img_to_array(img) / 255.0  # Rescale\n",
        "    \n",
        "    # Expand dimensions to match model input shape (batch size of 1)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    \n",
        "    return img_array\n",
        "\n",
        "# Function to make predictions\n",
        "def predict_image(img_path):\n",
        "    img_array = preprocess_image(img_path)\n",
        "    \n",
        "    # Get model prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    \n",
        "    # Get class with highest probability\n",
        "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
        "    \n",
        "    # Get class label\n",
        "    class_name = class_labels.get(predicted_class, \"Unknown\")\n",
        "    \n",
        "    print(f\"Predicted Class: {class_name}\")\n",
        "    return class_name\n",
        "\n",
        "# Test the model with an image\n",
        "image_path = \"image_411.jpg\"  # Change this to your test image path\n",
        "predicted_class = predict_image(image_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
